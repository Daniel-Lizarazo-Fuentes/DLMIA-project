{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "shm             128G     0  128G   0% /dev/shm\n"
     ]
    }
   ],
   "source": [
    "df -h /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export nnUNet_raw=\"/home/jovyan/nnUNet_raw\"\n",
    "export nnUNet_preprocessed=\"/home/jovyan/nnUNet_preprocessed\"\n",
    "export nnUNet_results=\"/home/jovyan/nnUNet_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nnUNet_raw\n",
      "/bin/bash\n"
     ]
    }
   ],
   "source": [
    "echo $nnUNet_raw\n",
    "echo $SHELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-03-18 09:31:27.051830: Using torch.compile...\n",
      "2025-03-18 09:31:29.560046: do_dummy_2d_data_aug: False\n",
      "2025-03-18 09:31:29.586096: Using splits from existing split file: /home/jovyan/nnUNet_preprocessed/Dataset100_normal/splits_final.json\n",
      "2025-03-18 09:31:29.597965: The split file contains 5 splits.\n",
      "2025-03-18 09:31:29.600004: Desired fold for training: 0\n",
      "2025-03-18 09:31:29.601647: This split has 16 training and 4 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [512.0, 512.0], 'spacing': [0.3818355, 0.3818355], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset100_normal', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.625, 0.3818355, 0.3818355], 'original_median_shape_after_transp': [210, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1592.0, 'mean': 386.3207092285156, 'median': 379.0, 'min': -370.0, 'percentile_00_5': 160.0, 'percentile_99_5': 668.0, 'std': 105.92166900634766}}} \n",
      "\n",
      "2025-03-18 09:31:33.163481: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-03-18 09:31:33.186676: \n",
      "2025-03-18 09:31:33.189381: Epoch 0\n",
      "2025-03-18 09:31:33.194447: Current learning rate: 0.01\n",
      "2025-03-18 09:37:05.935992: train_loss 0.0265\n",
      "2025-03-18 09:37:05.940123: val_loss -0.0487\n",
      "2025-03-18 09:37:05.942526: Pseudo dice [0.0001]\n",
      "2025-03-18 09:37:05.944602: Epoch time: 332.75 s\n",
      "2025-03-18 09:37:05.946441: Yayy! New best EMA pseudo Dice: 0.0001\n",
      "2025-03-18 09:37:08.222908: \n",
      "2025-03-18 09:37:08.225782: Epoch 1\n",
      "2025-03-18 09:37:08.228280: Current learning rate: 0.00999\n",
      "2025-03-18 09:40:30.092621: train_loss -0.2962\n",
      "2025-03-18 09:40:30.098281: val_loss -0.5174\n",
      "2025-03-18 09:40:30.099980: Pseudo dice [0.5813]\n",
      "2025-03-18 09:40:30.102514: Epoch time: 201.87 s\n",
      "2025-03-18 09:40:30.104830: Yayy! New best EMA pseudo Dice: 0.0582\n",
      "2025-03-18 09:40:32.426662: \n",
      "2025-03-18 09:40:32.429071: Epoch 2\n",
      "2025-03-18 09:40:32.431386: Current learning rate: 0.00998\n",
      "2025-03-18 09:43:53.994488: train_loss -0.5831\n",
      "2025-03-18 09:43:54.000326: val_loss -0.6465\n",
      "2025-03-18 09:43:54.002656: Pseudo dice [0.7021]\n",
      "2025-03-18 09:43:54.005122: Epoch time: 201.57 s\n",
      "2025-03-18 09:43:54.007418: Yayy! New best EMA pseudo Dice: 0.1226\n",
      "2025-03-18 09:43:56.293785: \n",
      "2025-03-18 09:43:56.296113: Epoch 3\n",
      "2025-03-18 09:43:56.298298: Current learning rate: 0.00997\n",
      "2025-03-18 09:47:17.717233: train_loss -0.674\n",
      "2025-03-18 09:47:17.722373: val_loss -0.7258\n",
      "2025-03-18 09:47:17.724146: Pseudo dice [0.78]\n",
      "2025-03-18 09:47:17.725578: Epoch time: 201.43 s\n",
      "2025-03-18 09:47:17.727803: Yayy! New best EMA pseudo Dice: 0.1883\n",
      "2025-03-18 09:47:19.964502: \n",
      "2025-03-18 09:47:19.965958: Epoch 4\n",
      "2025-03-18 09:47:19.967761: Current learning rate: 0.00996\n",
      "2025-03-18 09:50:41.359312: train_loss -0.7031\n",
      "2025-03-18 09:50:41.364703: val_loss -0.7112\n",
      "2025-03-18 09:50:41.366725: Pseudo dice [0.7582]\n",
      "2025-03-18 09:50:41.368554: Epoch time: 201.4 s\n",
      "2025-03-18 09:50:41.370669: Yayy! New best EMA pseudo Dice: 0.2453\n",
      "2025-03-18 09:50:43.682539: \n",
      "2025-03-18 09:50:43.684983: Epoch 5\n",
      "2025-03-18 09:50:43.686896: Current learning rate: 0.00995\n",
      "2025-03-18 09:54:05.104615: train_loss -0.7108\n",
      "2025-03-18 09:54:05.110753: val_loss -0.7035\n",
      "2025-03-18 09:54:05.113019: Pseudo dice [0.7512]\n",
      "2025-03-18 09:54:05.115339: Epoch time: 201.42 s\n",
      "2025-03-18 09:54:05.117441: Yayy! New best EMA pseudo Dice: 0.2959\n",
      "2025-03-18 09:54:07.399657: \n",
      "2025-03-18 09:54:07.402122: Epoch 6\n",
      "2025-03-18 09:54:07.404650: Current learning rate: 0.00995\n",
      "2025-03-18 09:57:28.932643: train_loss -0.7371\n",
      "2025-03-18 09:57:28.936470: val_loss -0.7592\n",
      "2025-03-18 09:57:28.939060: Pseudo dice [0.8043]\n",
      "2025-03-18 09:57:28.941401: Epoch time: 201.54 s\n",
      "2025-03-18 09:57:28.943913: Yayy! New best EMA pseudo Dice: 0.3468\n",
      "2025-03-18 09:57:31.178361: \n",
      "2025-03-18 09:57:31.181040: Epoch 7\n",
      "2025-03-18 09:57:31.184492: Current learning rate: 0.00994\n",
      "2025-03-18 10:00:52.672511: train_loss -0.7507\n",
      "2025-03-18 10:00:52.678153: val_loss -0.7409\n",
      "2025-03-18 10:00:52.680299: Pseudo dice [0.7812]\n",
      "2025-03-18 10:00:52.682623: Epoch time: 201.5 s\n",
      "2025-03-18 10:00:52.685044: Yayy! New best EMA pseudo Dice: 0.3902\n",
      "2025-03-18 10:00:54.989180: \n",
      "2025-03-18 10:00:54.992022: Epoch 8\n",
      "2025-03-18 10:00:54.993719: Current learning rate: 0.00993\n",
      "2025-03-18 10:04:16.549458: train_loss -0.7552\n",
      "2025-03-18 10:04:16.555207: val_loss -0.7708\n",
      "2025-03-18 10:04:16.557514: Pseudo dice [0.8067]\n",
      "2025-03-18 10:04:16.560044: Epoch time: 201.56 s\n",
      "2025-03-18 10:04:16.562257: Yayy! New best EMA pseudo Dice: 0.4319\n",
      "2025-03-18 10:04:18.821229: \n",
      "2025-03-18 10:04:18.823602: Epoch 9\n",
      "2025-03-18 10:04:18.825927: Current learning rate: 0.00992\n",
      "2025-03-18 10:07:40.347782: train_loss -0.7536\n",
      "2025-03-18 10:07:40.389699: val_loss -0.7727\n",
      "2025-03-18 10:07:40.407930: Pseudo dice [0.8031]\n",
      "2025-03-18 10:07:40.428096: Epoch time: 201.53 s\n",
      "2025-03-18 10:07:40.453349: Yayy! New best EMA pseudo Dice: 0.469\n",
      "2025-03-18 10:07:42.898558: \n",
      "2025-03-18 10:07:42.900943: Epoch 10\n",
      "2025-03-18 10:07:42.903380: Current learning rate: 0.00991\n",
      "2025-03-18 10:11:04.472976: train_loss -0.7558\n",
      "2025-03-18 10:11:04.478753: val_loss -0.7277\n",
      "2025-03-18 10:11:04.481187: Pseudo dice [0.7527]\n",
      "2025-03-18 10:11:04.483652: Epoch time: 201.58 s\n",
      "2025-03-18 10:11:04.486032: Yayy! New best EMA pseudo Dice: 0.4974\n",
      "2025-03-18 10:11:07.432033: \n",
      "2025-03-18 10:11:07.434373: Epoch 11\n",
      "2025-03-18 10:11:07.436660: Current learning rate: 0.0099\n",
      "2025-03-18 10:14:29.044935: train_loss -0.7716\n",
      "2025-03-18 10:14:29.051088: val_loss -0.767\n",
      "2025-03-18 10:14:29.053343: Pseudo dice [0.7942]\n",
      "2025-03-18 10:14:29.056017: Epoch time: 201.62 s\n",
      "2025-03-18 10:14:29.058602: Yayy! New best EMA pseudo Dice: 0.527\n",
      "2025-03-18 10:14:31.327736: \n",
      "2025-03-18 10:14:31.330410: Epoch 12\n",
      "2025-03-18 10:14:31.332928: Current learning rate: 0.00989\n",
      "2025-03-18 10:17:52.989089: train_loss -0.775\n",
      "2025-03-18 10:17:52.994643: val_loss -0.7622\n",
      "2025-03-18 10:17:52.996926: Pseudo dice [0.7938]\n",
      "2025-03-18 10:17:52.998863: Epoch time: 201.66 s\n",
      "2025-03-18 10:17:53.001173: Yayy! New best EMA pseudo Dice: 0.5537\n",
      "2025-03-18 10:17:55.246437: \n",
      "2025-03-18 10:17:55.249089: Epoch 13\n",
      "2025-03-18 10:17:55.251634: Current learning rate: 0.00988\n",
      "2025-03-18 10:21:16.837539: train_loss -0.7904\n",
      "2025-03-18 10:21:16.842120: val_loss -0.7803\n",
      "2025-03-18 10:21:16.843893: Pseudo dice [0.8078]\n",
      "2025-03-18 10:21:16.845540: Epoch time: 201.59 s\n",
      "2025-03-18 10:21:16.847246: Yayy! New best EMA pseudo Dice: 0.5791\n",
      "2025-03-18 10:21:19.138213: \n",
      "2025-03-18 10:21:19.140938: Epoch 14\n",
      "2025-03-18 10:21:19.143849: Current learning rate: 0.00987\n",
      "2025-03-18 10:24:40.815366: train_loss -0.8055\n",
      "2025-03-18 10:24:40.820925: val_loss -0.8118\n",
      "2025-03-18 10:24:40.823249: Pseudo dice [0.8357]\n",
      "2025-03-18 10:24:40.825210: Epoch time: 201.68 s\n",
      "2025-03-18 10:24:40.827368: Yayy! New best EMA pseudo Dice: 0.6048\n",
      "2025-03-18 10:24:43.044788: \n",
      "2025-03-18 10:24:43.047352: Epoch 15\n",
      "2025-03-18 10:24:43.049681: Current learning rate: 0.00986\n",
      "2025-03-18 10:28:04.777361: train_loss -0.8093\n",
      "2025-03-18 10:28:04.783826: val_loss -0.7913\n",
      "2025-03-18 10:28:04.786055: Pseudo dice [0.8179]\n",
      "2025-03-18 10:28:04.788410: Epoch time: 201.73 s\n",
      "2025-03-18 10:28:04.790557: Yayy! New best EMA pseudo Dice: 0.6261\n",
      "2025-03-18 10:28:07.119816: \n",
      "2025-03-18 10:28:07.123088: Epoch 16\n",
      "2025-03-18 10:28:07.125490: Current learning rate: 0.00986\n",
      "2025-03-18 10:31:28.979275: train_loss -0.8151\n",
      "2025-03-18 10:31:28.983999: val_loss -0.7835\n",
      "2025-03-18 10:31:28.985375: Pseudo dice [0.8087]\n",
      "2025-03-18 10:31:28.987189: Epoch time: 201.86 s\n",
      "2025-03-18 10:31:28.988914: Yayy! New best EMA pseudo Dice: 0.6444\n",
      "2025-03-18 10:31:31.367041: \n",
      "2025-03-18 10:31:31.369478: Epoch 17\n",
      "2025-03-18 10:31:31.371846: Current learning rate: 0.00985\n",
      "2025-03-18 10:34:53.041381: train_loss -0.8066\n",
      "2025-03-18 10:34:53.047416: val_loss -0.7748\n",
      "2025-03-18 10:34:53.050090: Pseudo dice [0.7996]\n",
      "2025-03-18 10:34:53.052753: Epoch time: 201.68 s\n",
      "2025-03-18 10:34:53.055217: Yayy! New best EMA pseudo Dice: 0.6599\n",
      "2025-03-18 10:34:55.267655: \n",
      "2025-03-18 10:34:55.270962: Epoch 18\n",
      "2025-03-18 10:34:55.273400: Current learning rate: 0.00984\n",
      "2025-03-18 10:38:16.923356: train_loss -0.8117\n",
      "2025-03-18 10:38:16.929265: val_loss -0.794\n",
      "2025-03-18 10:38:16.931193: Pseudo dice [0.8207]\n",
      "2025-03-18 10:38:16.933148: Epoch time: 201.66 s\n",
      "2025-03-18 10:38:16.935710: Yayy! New best EMA pseudo Dice: 0.676\n",
      "2025-03-18 10:38:19.299785: \n",
      "2025-03-18 10:38:19.302130: Epoch 19\n",
      "2025-03-18 10:38:19.304360: Current learning rate: 0.00983\n",
      "2025-03-18 10:41:40.994202: train_loss -0.8205\n",
      "2025-03-18 10:41:41.002089: val_loss -0.8\n",
      "2025-03-18 10:41:41.004183: Pseudo dice [0.8253]\n",
      "2025-03-18 10:41:41.006030: Epoch time: 201.7 s\n",
      "2025-03-18 10:41:41.008356: Yayy! New best EMA pseudo Dice: 0.6909\n",
      "2025-03-18 10:41:43.389267: \n",
      "2025-03-18 10:41:43.397472: Epoch 20\n",
      "2025-03-18 10:41:43.403189: Current learning rate: 0.00982\n",
      "2025-03-18 10:45:05.147455: train_loss -0.8397\n",
      "2025-03-18 10:45:05.151430: val_loss -0.7979\n",
      "2025-03-18 10:45:05.153583: Pseudo dice [0.8235]\n",
      "2025-03-18 10:45:05.156248: Epoch time: 201.76 s\n",
      "2025-03-18 10:45:05.157929: Yayy! New best EMA pseudo Dice: 0.7042\n",
      "2025-03-18 10:45:07.496600: \n",
      "2025-03-18 10:45:07.499249: Epoch 21\n",
      "2025-03-18 10:45:07.501657: Current learning rate: 0.00981\n",
      "2025-03-18 10:48:29.147770: train_loss -0.8165\n",
      "2025-03-18 10:48:29.151913: val_loss -0.7921\n",
      "2025-03-18 10:48:29.154175: Pseudo dice [0.8166]\n",
      "2025-03-18 10:48:29.156720: Epoch time: 201.65 s\n",
      "2025-03-18 10:48:29.158999: Yayy! New best EMA pseudo Dice: 0.7154\n",
      "2025-03-18 10:48:31.469187: \n",
      "2025-03-18 10:48:31.472097: Epoch 22\n",
      "2025-03-18 10:48:31.474109: Current learning rate: 0.0098\n",
      "2025-03-18 10:51:53.114277: train_loss -0.8347\n",
      "2025-03-18 10:51:53.119836: val_loss -0.827\n",
      "2025-03-18 10:51:53.122046: Pseudo dice [0.8511]\n",
      "2025-03-18 10:51:53.124534: Epoch time: 201.65 s\n",
      "2025-03-18 10:51:53.126774: Yayy! New best EMA pseudo Dice: 0.729\n",
      "2025-03-18 10:51:55.313417: \n",
      "2025-03-18 10:51:55.316068: Epoch 23\n",
      "2025-03-18 10:51:55.318721: Current learning rate: 0.00979\n",
      "2025-03-18 10:55:16.947767: train_loss -0.8303\n",
      "2025-03-18 10:55:16.952735: val_loss -0.8134\n",
      "2025-03-18 10:55:16.955389: Pseudo dice [0.8302]\n",
      "2025-03-18 10:55:16.957973: Epoch time: 201.64 s\n",
      "2025-03-18 10:55:16.960397: Yayy! New best EMA pseudo Dice: 0.7391\n",
      "2025-03-18 10:55:19.281156: \n",
      "2025-03-18 10:55:19.283624: Epoch 24\n",
      "2025-03-18 10:55:19.286008: Current learning rate: 0.00978\n",
      "2025-03-18 10:58:41.081639: train_loss -0.8309\n",
      "2025-03-18 10:58:41.087231: val_loss -0.8231\n",
      "2025-03-18 10:58:41.089302: Pseudo dice [0.85]\n",
      "2025-03-18 10:58:41.091555: Epoch time: 201.8 s\n",
      "2025-03-18 10:58:41.093464: Yayy! New best EMA pseudo Dice: 0.7502\n",
      "2025-03-18 10:58:43.790307: \n",
      "2025-03-18 10:58:43.792790: Epoch 25\n",
      "2025-03-18 10:58:43.795003: Current learning rate: 0.00977\n",
      "2025-03-18 11:02:05.499561: train_loss -0.8289\n",
      "2025-03-18 11:02:05.503905: val_loss -0.8016\n",
      "2025-03-18 11:02:05.506394: Pseudo dice [0.8245]\n",
      "2025-03-18 11:02:05.508898: Epoch time: 201.71 s\n",
      "2025-03-18 11:02:05.511512: Yayy! New best EMA pseudo Dice: 0.7576\n",
      "2025-03-18 11:02:07.817673: \n",
      "2025-03-18 11:02:07.820774: Epoch 26\n",
      "2025-03-18 11:02:07.823075: Current learning rate: 0.00977\n",
      "2025-03-18 11:05:29.652479: train_loss -0.8335\n",
      "2025-03-18 11:05:29.661956: val_loss -0.829\n",
      "2025-03-18 11:05:29.671005: Pseudo dice [0.8509]\n",
      "2025-03-18 11:05:29.684762: Epoch time: 201.84 s\n",
      "2025-03-18 11:05:29.694167: Yayy! New best EMA pseudo Dice: 0.7669\n",
      "2025-03-18 11:05:32.094884: \n",
      "2025-03-18 11:05:32.098840: Epoch 27\n",
      "2025-03-18 11:05:32.101453: Current learning rate: 0.00976\n",
      "2025-03-18 11:08:54.062459: train_loss -0.8427\n",
      "2025-03-18 11:08:54.067420: val_loss -0.8156\n",
      "2025-03-18 11:08:54.069908: Pseudo dice [0.8372]\n",
      "2025-03-18 11:08:54.072307: Epoch time: 201.97 s\n",
      "2025-03-18 11:08:54.074418: Yayy! New best EMA pseudo Dice: 0.774\n",
      "2025-03-18 11:08:56.311831: \n",
      "2025-03-18 11:08:56.314889: Epoch 28\n",
      "2025-03-18 11:08:56.317020: Current learning rate: 0.00975\n",
      "2025-03-18 11:12:18.233758: train_loss -0.8365\n",
      "2025-03-18 11:12:18.240909: val_loss -0.7851\n",
      "2025-03-18 11:12:18.243102: Pseudo dice [0.8032]\n",
      "2025-03-18 11:12:18.247010: Epoch time: 201.92 s\n",
      "2025-03-18 11:12:18.249176: Yayy! New best EMA pseudo Dice: 0.7769\n",
      "2025-03-18 11:12:20.530451: \n",
      "2025-03-18 11:12:20.532938: Epoch 29\n",
      "2025-03-18 11:12:20.535558: Current learning rate: 0.00974\n",
      "2025-03-18 11:15:42.493838: train_loss -0.8524\n",
      "2025-03-18 11:15:42.498156: val_loss -0.8242\n",
      "2025-03-18 11:15:42.500133: Pseudo dice [0.8445]\n",
      "2025-03-18 11:15:42.501841: Epoch time: 201.97 s\n",
      "2025-03-18 11:15:42.503689: Yayy! New best EMA pseudo Dice: 0.7836\n",
      "2025-03-18 11:15:44.758448: \n",
      "2025-03-18 11:15:44.760991: Epoch 30\n",
      "2025-03-18 11:15:44.763080: Current learning rate: 0.00973\n",
      "2025-03-18 11:19:06.553032: train_loss -0.8452\n",
      "2025-03-18 11:19:06.558297: val_loss -0.799\n",
      "2025-03-18 11:19:06.560755: Pseudo dice [0.8193]\n",
      "2025-03-18 11:19:06.563371: Epoch time: 201.8 s\n",
      "2025-03-18 11:19:06.565635: Yayy! New best EMA pseudo Dice: 0.7872\n",
      "2025-03-18 11:19:08.863797: \n",
      "2025-03-18 11:19:08.866395: Epoch 31\n",
      "2025-03-18 11:19:08.868992: Current learning rate: 0.00972\n",
      "2025-03-18 11:22:30.809767: train_loss -0.8493\n",
      "2025-03-18 11:22:30.814183: val_loss -0.8157\n",
      "2025-03-18 11:22:30.816257: Pseudo dice [0.8303]\n",
      "2025-03-18 11:22:30.818113: Epoch time: 201.95 s\n",
      "2025-03-18 11:22:30.820070: Yayy! New best EMA pseudo Dice: 0.7915\n",
      "2025-03-18 11:22:33.103377: \n",
      "2025-03-18 11:22:33.106027: Epoch 32\n",
      "2025-03-18 11:22:33.108295: Current learning rate: 0.00971\n",
      "2025-03-18 11:25:55.021502: train_loss -0.8462\n",
      "2025-03-18 11:25:55.026628: val_loss -0.8379\n",
      "2025-03-18 11:25:55.029027: Pseudo dice [0.8553]\n",
      "2025-03-18 11:25:55.031060: Epoch time: 201.92 s\n",
      "2025-03-18 11:25:55.033512: Yayy! New best EMA pseudo Dice: 0.7979\n",
      "2025-03-18 11:25:57.326300: \n",
      "2025-03-18 11:25:57.329127: Epoch 33\n",
      "2025-03-18 11:25:57.332083: Current learning rate: 0.0097\n",
      "2025-03-18 11:29:19.219200: train_loss -0.8501\n",
      "2025-03-18 11:29:19.224298: val_loss -0.7998\n",
      "2025-03-18 11:29:19.226763: Pseudo dice [0.8259]\n",
      "2025-03-18 11:29:19.229175: Epoch time: 201.89 s\n",
      "2025-03-18 11:29:19.233002: Yayy! New best EMA pseudo Dice: 0.8007\n",
      "2025-03-18 11:29:21.518205: \n",
      "2025-03-18 11:29:21.520676: Epoch 34\n",
      "2025-03-18 11:29:21.523010: Current learning rate: 0.00969\n",
      "2025-03-18 11:32:44.140928: train_loss -0.8549\n",
      "2025-03-18 11:32:44.147252: val_loss -0.845\n",
      "2025-03-18 11:32:44.149890: Pseudo dice [0.8627]\n",
      "2025-03-18 11:32:44.152406: Epoch time: 202.63 s\n",
      "2025-03-18 11:32:44.154994: Yayy! New best EMA pseudo Dice: 0.8069\n",
      "2025-03-18 11:32:46.472476: \n",
      "2025-03-18 11:32:46.476070: Epoch 35\n",
      "2025-03-18 11:32:46.478836: Current learning rate: 0.00968\n",
      "2025-03-18 11:36:31.377780: train_loss -0.8589\n",
      "2025-03-18 11:36:31.380430: val_loss -0.8228\n",
      "2025-03-18 11:36:31.382766: Pseudo dice [0.8401]\n",
      "2025-03-18 11:36:31.385003: Epoch time: 224.91 s\n",
      "2025-03-18 11:36:31.387444: Yayy! New best EMA pseudo Dice: 0.8102\n",
      "2025-03-18 11:36:33.729410: \n",
      "2025-03-18 11:36:33.731657: Epoch 36\n",
      "2025-03-18 11:36:33.733596: Current learning rate: 0.00968\n",
      "2025-03-18 11:39:59.757229: train_loss -0.8656\n",
      "2025-03-18 11:39:59.761832: val_loss -0.8142\n",
      "2025-03-18 11:39:59.764546: Pseudo dice [0.8293]\n",
      "2025-03-18 11:39:59.767133: Epoch time: 206.03 s\n",
      "2025-03-18 11:39:59.769438: Yayy! New best EMA pseudo Dice: 0.8121\n",
      "2025-03-18 11:40:02.087442: \n",
      "2025-03-18 11:40:02.089760: Epoch 37\n",
      "2025-03-18 11:40:02.092095: Current learning rate: 0.00967\n",
      "2025-03-18 11:43:49.073386: train_loss -0.8659\n",
      "2025-03-18 11:43:49.076421: val_loss -0.8533\n",
      "2025-03-18 11:43:49.077771: Pseudo dice [0.8652]\n",
      "2025-03-18 11:43:49.079013: Epoch time: 226.99 s\n",
      "2025-03-18 11:43:49.080261: Yayy! New best EMA pseudo Dice: 0.8174\n",
      "2025-03-18 11:43:51.388424: \n",
      "2025-03-18 11:43:51.390034: Epoch 38\n",
      "2025-03-18 11:43:51.391581: Current learning rate: 0.00966\n",
      "2025-03-18 11:47:22.298325: train_loss -0.862\n",
      "2025-03-18 11:47:22.300704: val_loss -0.8185\n",
      "2025-03-18 11:47:22.302662: Pseudo dice [0.835]\n",
      "2025-03-18 11:47:22.304943: Epoch time: 210.91 s\n",
      "2025-03-18 11:47:22.306997: Yayy! New best EMA pseudo Dice: 0.8192\n",
      "2025-03-18 11:47:25.126824: \n",
      "2025-03-18 11:47:25.129839: Epoch 39\n",
      "2025-03-18 11:47:25.132275: Current learning rate: 0.00965\n",
      "2025-03-18 11:50:47.248863: train_loss -0.8639\n",
      "2025-03-18 11:50:47.254039: val_loss -0.8233\n",
      "2025-03-18 11:50:47.256363: Pseudo dice [0.8369]\n",
      "2025-03-18 11:50:47.259317: Epoch time: 202.12 s\n",
      "2025-03-18 11:50:47.261897: Yayy! New best EMA pseudo Dice: 0.821\n",
      "2025-03-18 11:50:49.661258: \n",
      "2025-03-18 11:50:49.663846: Epoch 40\n",
      "2025-03-18 11:50:49.666070: Current learning rate: 0.00964\n",
      "2025-03-18 11:54:11.876909: train_loss -0.8678\n",
      "2025-03-18 11:54:11.882613: val_loss -0.8282\n",
      "2025-03-18 11:54:11.885073: Pseudo dice [0.8449]\n",
      "2025-03-18 11:54:11.887352: Epoch time: 202.22 s\n",
      "2025-03-18 11:54:11.889540: Yayy! New best EMA pseudo Dice: 0.8233\n",
      "2025-03-18 11:54:14.234153: \n",
      "2025-03-18 11:54:14.236277: Epoch 41\n",
      "2025-03-18 11:54:14.238266: Current learning rate: 0.00963\n",
      "2025-03-18 11:57:36.303690: train_loss -0.8669\n",
      "2025-03-18 11:57:36.309933: val_loss -0.835\n",
      "2025-03-18 11:57:36.312465: Pseudo dice [0.8536]\n",
      "2025-03-18 11:57:36.315255: Epoch time: 202.07 s\n",
      "2025-03-18 11:57:36.317589: Yayy! New best EMA pseudo Dice: 0.8264\n",
      "2025-03-18 11:57:38.658352: \n",
      "2025-03-18 11:57:38.661618: Epoch 42\n",
      "2025-03-18 11:57:38.664718: Current learning rate: 0.00962\n",
      "2025-03-18 12:01:01.003852: train_loss -0.8721\n",
      "2025-03-18 12:01:01.006619: val_loss -0.8396\n",
      "2025-03-18 12:01:01.009100: Pseudo dice [0.8532]\n",
      "2025-03-18 12:01:01.012180: Epoch time: 202.35 s\n",
      "2025-03-18 12:01:01.015028: Yayy! New best EMA pseudo Dice: 0.8291\n",
      "2025-03-18 12:01:03.327141: \n",
      "2025-03-18 12:01:03.330044: Epoch 43\n",
      "2025-03-18 12:01:03.332346: Current learning rate: 0.00961\n",
      "2025-03-18 12:04:25.651781: train_loss -0.8582\n",
      "2025-03-18 12:04:25.655795: val_loss -0.8183\n",
      "2025-03-18 12:04:25.657125: Pseudo dice [0.8307]\n",
      "2025-03-18 12:04:25.658486: Epoch time: 202.33 s\n",
      "2025-03-18 12:04:25.660047: Yayy! New best EMA pseudo Dice: 0.8292\n",
      "2025-03-18 12:04:28.039313: \n",
      "2025-03-18 12:04:28.040991: Epoch 44\n",
      "2025-03-18 12:04:28.042335: Current learning rate: 0.0096\n",
      "2025-03-18 12:07:50.206155: train_loss -0.8508\n",
      "2025-03-18 12:07:50.209667: val_loss -0.8139\n",
      "2025-03-18 12:07:50.211107: Pseudo dice [0.8279]\n",
      "2025-03-18 12:07:50.212724: Epoch time: 202.17 s\n",
      "2025-03-18 12:07:51.846393: \n",
      "2025-03-18 12:07:51.848542: Epoch 45\n",
      "2025-03-18 12:07:51.849751: Current learning rate: 0.00959\n",
      "2025-03-18 12:11:14.127783: train_loss -0.8683\n",
      "2025-03-18 12:11:14.139044: val_loss -0.8278\n",
      "2025-03-18 12:11:14.141171: Pseudo dice [0.8422]\n",
      "2025-03-18 12:11:14.143173: Epoch time: 202.28 s\n",
      "2025-03-18 12:11:14.144901: Yayy! New best EMA pseudo Dice: 0.8304\n",
      "2025-03-18 12:11:16.406772: \n",
      "2025-03-18 12:11:16.409447: Epoch 46\n",
      "2025-03-18 12:11:16.412170: Current learning rate: 0.00959\n",
      "2025-03-18 12:14:38.335900: train_loss -0.8743\n",
      "2025-03-18 12:14:38.339810: val_loss -0.8347\n",
      "2025-03-18 12:14:38.341407: Pseudo dice [0.8458]\n",
      "2025-03-18 12:14:38.343531: Epoch time: 201.93 s\n",
      "2025-03-18 12:14:38.344862: Yayy! New best EMA pseudo Dice: 0.8319\n",
      "2025-03-18 12:14:40.550966: \n",
      "2025-03-18 12:14:40.553646: Epoch 47\n",
      "2025-03-18 12:14:40.555594: Current learning rate: 0.00958\n",
      "2025-03-18 12:18:02.513088: train_loss -0.8775\n",
      "2025-03-18 12:18:02.516935: val_loss -0.8248\n",
      "2025-03-18 12:18:02.518188: Pseudo dice [0.8398]\n",
      "2025-03-18 12:18:02.519443: Epoch time: 201.96 s\n",
      "2025-03-18 12:18:02.520689: Yayy! New best EMA pseudo Dice: 0.8327\n",
      "2025-03-18 12:18:04.700894: \n",
      "2025-03-18 12:18:04.704057: Epoch 48\n",
      "2025-03-18 12:18:04.706835: Current learning rate: 0.00957\n",
      "2025-03-18 12:21:26.637054: train_loss -0.8682\n",
      "2025-03-18 12:21:26.640390: val_loss -0.8324\n",
      "2025-03-18 12:21:26.641759: Pseudo dice [0.8458]\n",
      "2025-03-18 12:21:26.643212: Epoch time: 201.94 s\n",
      "2025-03-18 12:21:26.644504: Yayy! New best EMA pseudo Dice: 0.834\n",
      "2025-03-18 12:21:28.960542: \n",
      "2025-03-18 12:21:28.962030: Epoch 49\n",
      "2025-03-18 12:21:28.963367: Current learning rate: 0.00956\n",
      "2025-03-18 12:24:50.883620: train_loss -0.8705\n",
      "2025-03-18 12:24:50.886905: val_loss -0.8511\n",
      "2025-03-18 12:24:50.888397: Pseudo dice [0.8685]\n",
      "2025-03-18 12:24:50.889847: Epoch time: 201.93 s\n",
      "2025-03-18 12:24:51.703422: Yayy! New best EMA pseudo Dice: 0.8375\n",
      "2025-03-18 12:24:54.026883: \n",
      "2025-03-18 12:24:54.028518: Epoch 50\n",
      "2025-03-18 12:24:54.030077: Current learning rate: 0.00955\n",
      "2025-03-18 12:28:15.805678: train_loss -0.8772\n",
      "2025-03-18 12:28:15.809740: val_loss -0.8444\n",
      "2025-03-18 12:28:15.811152: Pseudo dice [0.8592]\n",
      "2025-03-18 12:28:15.812476: Epoch time: 201.78 s\n",
      "2025-03-18 12:28:15.814123: Yayy! New best EMA pseudo Dice: 0.8397\n",
      "2025-03-18 12:28:18.081663: \n",
      "2025-03-18 12:28:18.083190: Epoch 51\n",
      "2025-03-18 12:28:18.084573: Current learning rate: 0.00954\n",
      "2025-03-18 12:31:39.903503: train_loss -0.8674\n",
      "2025-03-18 12:31:39.907056: val_loss -0.8113\n",
      "2025-03-18 12:31:39.908591: Pseudo dice [0.826]\n",
      "2025-03-18 12:31:39.910253: Epoch time: 201.82 s\n",
      "2025-03-18 12:31:41.312822: \n",
      "2025-03-18 12:31:41.314364: Epoch 52\n",
      "2025-03-18 12:31:41.315817: Current learning rate: 0.00953\n",
      "2025-03-18 12:35:03.156244: train_loss -0.8701\n",
      "2025-03-18 12:35:03.160123: val_loss -0.8365\n",
      "2025-03-18 12:35:03.162306: Pseudo dice [0.849]\n",
      "2025-03-18 12:35:03.163857: Epoch time: 201.85 s\n",
      "2025-03-18 12:35:04.550466: \n",
      "2025-03-18 12:35:04.552737: Epoch 53\n",
      "2025-03-18 12:35:04.554340: Current learning rate: 0.00952\n",
      "2025-03-18 12:38:26.380959: train_loss -0.875\n",
      "2025-03-18 12:38:26.385160: val_loss -0.829\n",
      "2025-03-18 12:38:26.386719: Pseudo dice [0.844]\n",
      "2025-03-18 12:38:26.388170: Epoch time: 201.83 s\n",
      "2025-03-18 12:38:26.389617: Yayy! New best EMA pseudo Dice: 0.8398\n",
      "2025-03-18 12:38:29.285630: \n",
      "2025-03-18 12:38:29.289213: Epoch 54\n",
      "2025-03-18 12:38:29.292793: Current learning rate: 0.00951\n",
      "2025-03-18 12:41:51.207414: train_loss -0.869\n",
      "2025-03-18 12:41:51.211340: val_loss -0.8269\n",
      "2025-03-18 12:41:51.213681: Pseudo dice [0.8408]\n",
      "2025-03-18 12:41:51.215212: Epoch time: 201.92 s\n",
      "2025-03-18 12:41:51.216711: Yayy! New best EMA pseudo Dice: 0.8399\n",
      "2025-03-18 12:41:53.579427: \n",
      "2025-03-18 12:41:53.582090: Epoch 55\n",
      "2025-03-18 12:41:53.584033: Current learning rate: 0.0095\n",
      "2025-03-18 12:45:15.534478: train_loss -0.8722\n",
      "2025-03-18 12:45:15.537723: val_loss -0.8406\n",
      "2025-03-18 12:45:15.539157: Pseudo dice [0.8575]\n",
      "2025-03-18 12:45:15.540534: Epoch time: 201.96 s\n",
      "2025-03-18 12:45:15.541854: Yayy! New best EMA pseudo Dice: 0.8417\n",
      "2025-03-18 12:45:17.797407: \n",
      "2025-03-18 12:45:17.798841: Epoch 56\n",
      "2025-03-18 12:45:17.800317: Current learning rate: 0.00949\n",
      "2025-03-18 12:48:42.115228: train_loss -0.8766\n",
      "2025-03-18 12:48:42.118892: val_loss -0.8392\n",
      "2025-03-18 12:48:42.120351: Pseudo dice [0.8516]\n",
      "2025-03-18 12:48:42.121923: Epoch time: 204.32 s\n",
      "2025-03-18 12:48:42.123381: Yayy! New best EMA pseudo Dice: 0.8427\n",
      "2025-03-18 12:48:44.312315: \n",
      "2025-03-18 12:48:44.315266: Epoch 57\n",
      "2025-03-18 12:48:44.317541: Current learning rate: 0.00949\n",
      "2025-03-18 12:52:16.199420: train_loss -0.8807\n",
      "2025-03-18 12:52:16.203159: val_loss -0.8411\n",
      "2025-03-18 12:52:16.204484: Pseudo dice [0.8571]\n",
      "2025-03-18 12:52:16.205822: Epoch time: 211.89 s\n",
      "2025-03-18 12:52:16.207160: Yayy! New best EMA pseudo Dice: 0.8441\n",
      "2025-03-18 12:52:18.421080: \n",
      "2025-03-18 12:52:18.422472: Epoch 58\n",
      "2025-03-18 12:52:18.423889: Current learning rate: 0.00948\n",
      "2025-03-18 12:55:40.955133: train_loss -0.8778\n",
      "2025-03-18 12:55:40.960500: val_loss -0.8449\n",
      "2025-03-18 12:55:40.963199: Pseudo dice [0.8591]\n",
      "2025-03-18 12:55:40.966005: Epoch time: 202.54 s\n",
      "2025-03-18 12:55:40.968890: Yayy! New best EMA pseudo Dice: 0.8456\n",
      "2025-03-18 12:55:43.307887: \n",
      "2025-03-18 12:55:43.310096: Epoch 59\n",
      "2025-03-18 12:55:43.311810: Current learning rate: 0.00947\n",
      "2025-03-18 12:59:05.904071: train_loss -0.884\n",
      "2025-03-18 12:59:05.907575: val_loss -0.8573\n",
      "2025-03-18 12:59:05.909056: Pseudo dice [0.8719]\n",
      "2025-03-18 12:59:05.910422: Epoch time: 202.6 s\n",
      "2025-03-18 12:59:05.911735: Yayy! New best EMA pseudo Dice: 0.8482\n",
      "2025-03-18 12:59:08.265516: \n",
      "2025-03-18 12:59:08.266971: Epoch 60\n",
      "2025-03-18 12:59:08.268352: Current learning rate: 0.00946\n",
      "2025-03-18 13:02:30.817134: train_loss -0.8867\n",
      "2025-03-18 13:02:30.821027: val_loss -0.8444\n",
      "2025-03-18 13:02:30.822517: Pseudo dice [0.8568]\n",
      "2025-03-18 13:02:30.824089: Epoch time: 202.55 s\n",
      "2025-03-18 13:02:30.825749: Yayy! New best EMA pseudo Dice: 0.8491\n",
      "2025-03-18 13:02:33.333336: \n",
      "2025-03-18 13:02:33.334753: Epoch 61\n",
      "2025-03-18 13:02:33.336177: Current learning rate: 0.00945\n",
      "2025-03-18 13:05:55.993009: train_loss -0.8874\n",
      "2025-03-18 13:05:55.998062: val_loss -0.8306\n",
      "2025-03-18 13:05:56.000479: Pseudo dice [0.8428]\n",
      "2025-03-18 13:05:56.002745: Epoch time: 202.66 s\n",
      "2025-03-18 13:05:57.487113: \n",
      "2025-03-18 13:05:57.490041: Epoch 62\n",
      "2025-03-18 13:05:57.493264: Current learning rate: 0.00944\n",
      "2025-03-18 13:09:20.200846: train_loss -0.8745\n",
      "2025-03-18 13:09:20.207332: val_loss -0.8297\n",
      "2025-03-18 13:09:20.210585: Pseudo dice [0.8439]\n",
      "2025-03-18 13:09:20.213440: Epoch time: 202.72 s\n",
      "2025-03-18 13:09:21.688693: \n",
      "2025-03-18 13:09:21.690700: Epoch 63\n",
      "2025-03-18 13:09:21.692269: Current learning rate: 0.00943\n",
      "2025-03-18 13:12:44.276949: train_loss -0.8812\n",
      "2025-03-18 13:12:44.380352: val_loss -0.836\n",
      "2025-03-18 13:12:44.386404: Pseudo dice [0.8484]\n",
      "2025-03-18 13:12:44.395717: Epoch time: 202.59 s\n",
      "2025-03-18 13:12:45.877255: \n",
      "2025-03-18 13:12:45.880015: Epoch 64\n",
      "2025-03-18 13:12:45.883984: Current learning rate: 0.00942\n",
      "2025-03-18 13:16:14.541200: train_loss -0.8829\n",
      "2025-03-18 13:16:14.544712: val_loss -0.8378\n",
      "2025-03-18 13:16:14.546490: Pseudo dice [0.8544]\n",
      "2025-03-18 13:16:14.548211: Epoch time: 208.67 s\n",
      "2025-03-18 13:16:16.037600: \n",
      "2025-03-18 13:16:16.039061: Epoch 65\n",
      "2025-03-18 13:16:16.040437: Current learning rate: 0.00941\n",
      "2025-03-18 13:19:38.388294: train_loss -0.8789\n",
      "2025-03-18 13:19:38.391794: val_loss -0.8209\n",
      "2025-03-18 13:19:38.393191: Pseudo dice [0.8334]\n",
      "2025-03-18 13:19:38.394571: Epoch time: 202.35 s\n",
      "2025-03-18 13:19:39.784096: \n",
      "2025-03-18 13:19:39.785562: Epoch 66\n",
      "2025-03-18 13:19:39.786964: Current learning rate: 0.0094\n",
      "2025-03-18 13:23:02.033203: train_loss -0.8853\n",
      "2025-03-18 13:23:02.036511: val_loss -0.8294\n",
      "2025-03-18 13:23:02.037983: Pseudo dice [0.844]\n",
      "2025-03-18 13:23:02.039433: Epoch time: 202.25 s\n",
      "2025-03-18 13:23:03.588104: \n",
      "2025-03-18 13:23:03.589605: Epoch 67\n",
      "2025-03-18 13:23:03.590943: Current learning rate: 0.00939\n",
      "2025-03-18 13:26:25.946417: train_loss -0.8707\n",
      "2025-03-18 13:26:25.949666: val_loss -0.8079\n",
      "2025-03-18 13:26:25.951110: Pseudo dice [0.8248]\n",
      "2025-03-18 13:26:25.952462: Epoch time: 202.36 s\n",
      "2025-03-18 13:26:28.018853: \n",
      "2025-03-18 13:26:28.020887: Epoch 68\n",
      "2025-03-18 13:26:28.022444: Current learning rate: 0.00939\n",
      "2025-03-18 13:29:50.403378: train_loss -0.8622\n",
      "2025-03-18 13:29:50.408992: val_loss -0.8027\n",
      "2025-03-18 13:29:50.410297: Pseudo dice [0.8175]\n",
      "2025-03-18 13:29:50.412185: Epoch time: 202.39 s\n",
      "2025-03-18 13:29:51.772821: \n",
      "2025-03-18 13:29:51.789883: Epoch 69\n",
      "2025-03-18 13:29:51.791914: Current learning rate: 0.00938\n",
      "2025-03-18 13:33:14.167284: train_loss -0.8584\n",
      "2025-03-18 13:33:14.171852: val_loss -0.8186\n",
      "2025-03-18 13:33:14.173261: Pseudo dice [0.8334]\n",
      "2025-03-18 13:33:14.174669: Epoch time: 202.4 s\n",
      "2025-03-18 13:33:15.664877: \n",
      "2025-03-18 13:33:15.666307: Epoch 70\n",
      "2025-03-18 13:33:15.667937: Current learning rate: 0.00937\n",
      "2025-03-18 13:36:38.042473: train_loss -0.8696\n",
      "2025-03-18 13:36:38.045733: val_loss -0.8278\n",
      "2025-03-18 13:36:38.047176: Pseudo dice [0.8402]\n",
      "2025-03-18 13:36:38.049998: Epoch time: 202.38 s\n",
      "2025-03-18 13:36:39.532177: \n",
      "2025-03-18 13:36:39.533653: Epoch 71\n",
      "2025-03-18 13:36:39.535444: Current learning rate: 0.00936\n",
      "2025-03-18 13:40:01.947600: train_loss -0.872\n",
      "2025-03-18 13:40:01.950769: val_loss -0.8179\n",
      "2025-03-18 13:40:01.952025: Pseudo dice [0.8348]\n",
      "2025-03-18 13:40:01.953317: Epoch time: 202.42 s\n",
      "2025-03-18 13:40:03.470644: \n",
      "2025-03-18 13:40:03.471961: Epoch 72\n",
      "2025-03-18 13:40:03.473260: Current learning rate: 0.00935\n",
      "2025-03-18 13:43:25.902991: train_loss -0.8684\n",
      "2025-03-18 13:43:25.905145: val_loss -0.8413\n",
      "2025-03-18 13:43:25.906532: Pseudo dice [0.8558]\n",
      "2025-03-18 13:43:25.907909: Epoch time: 202.43 s\n",
      "2025-03-18 13:43:27.402085: \n",
      "2025-03-18 13:43:27.403787: Epoch 73\n",
      "2025-03-18 13:43:27.405447: Current learning rate: 0.00934\n",
      "2025-03-18 13:46:50.066498: train_loss -0.8808\n",
      "2025-03-18 13:46:50.069718: val_loss -0.8313\n",
      "2025-03-18 13:46:50.071421: Pseudo dice [0.8477]\n",
      "2025-03-18 13:46:50.073009: Epoch time: 202.67 s\n",
      "2025-03-18 13:46:51.505144: \n",
      "2025-03-18 13:46:51.507316: Epoch 74\n",
      "2025-03-18 13:46:51.509251: Current learning rate: 0.00933\n",
      "2025-03-18 13:50:14.128011: train_loss -0.886\n",
      "2025-03-18 13:50:14.133444: val_loss -0.8404\n",
      "2025-03-18 13:50:14.137503: Pseudo dice [0.8544]\n",
      "2025-03-18 13:50:14.140196: Epoch time: 202.63 s\n",
      "2025-03-18 13:50:15.647301: \n",
      "2025-03-18 13:50:15.650122: Epoch 75\n",
      "2025-03-18 13:50:15.653007: Current learning rate: 0.00932\n",
      "2025-03-18 13:53:38.283321: train_loss -0.8867\n",
      "2025-03-18 13:53:38.286548: val_loss -0.8407\n",
      "2025-03-18 13:53:38.289502: Pseudo dice [0.8571]\n",
      "2025-03-18 13:53:38.291545: Epoch time: 202.64 s\n",
      "2025-03-18 13:53:39.733920: \n",
      "2025-03-18 13:53:39.737465: Epoch 76\n",
      "2025-03-18 13:53:39.740950: Current learning rate: 0.00931\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/bin/nnUNetv2_train\", line 8, in <module>\n",
      "    sys.exit(run_training_entry())\n",
      "  File \"/home/jovyan/.local/lib/python3.10/site-packages/nnunetv2/run/run_training.py\", line 267, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"/home/jovyan/.local/lib/python3.10/site-packages/nnunetv2/run/run_training.py\", line 207, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"/home/jovyan/.local/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1371, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "  File \"/home/jovyan/.local/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 997, in train_step\n",
      "    self.grad_scaler.step(self.optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\", line 453, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\", line 350, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\", line 350, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "export CUDA_VISIBLE_DEVICES=1 \n",
    ".local/bin/nnUNetv2_train -device \"cuda\" 100 2d 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-03-18 13:57:02.466882: Using torch.compile...\n",
      "2025-03-18 13:57:03.492599: do_dummy_2d_data_aug: False\n",
      "2025-03-18 13:57:03.498760: Using splits from existing split file: /home/jovyan/nnUNet_preprocessed/Dataset100_normal/splits_final.json\n",
      "2025-03-18 13:57:03.504534: The split file contains 5 splits.\n",
      "2025-03-18 13:57:03.506372: Desired fold for training: 0\n",
      "2025-03-18 13:57:03.507873: This split has 16 training and 4 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [64, 192, 192], 'median_image_size_in_voxels': [210.0, 512.0, 512.0], 'spacing': [0.625, 0.3818355, 0.3818355], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset100_normal', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.625, 0.3818355, 0.3818355], 'original_median_shape_after_transp': [210, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1592.0, 'mean': 386.3207092285156, 'median': 379.0, 'min': -370.0, 'percentile_00_5': 160.0, 'percentile_99_5': 668.0, 'std': 105.92166900634766}}} \n",
      "\n",
      "2025-03-18 13:57:07.856986: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-03-18 13:57:07.878183: \n",
      "2025-03-18 13:57:07.880445: Epoch 0\n",
      "2025-03-18 13:57:07.882652: Current learning rate: 0.01\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-03-18 13:57:12.306469: Using torch.compile...\n",
      "2025-03-18 13:57:13.142401: do_dummy_2d_data_aug: False\n",
      "2025-03-18 13:57:13.147706: Using splits from existing split file: /home/jovyan/nnUNet_preprocessed/Dataset100_normal/splits_final.json\n",
      "2025-03-18 13:57:13.152660: The split file contains 5 splits.\n",
      "2025-03-18 13:57:13.154899: Desired fold for training: 0\n",
      "2025-03-18 13:57:13.157436: This split has 16 training and 4 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [64, 192, 192], 'median_image_size_in_voxels': [210.0, 512.0, 512.0], 'spacing': [0.625, 0.3818355, 0.3818355], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset100_normal', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.625, 0.3818355, 0.3818355], 'original_median_shape_after_transp': [210, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1592.0, 'mean': 386.3207092285156, 'median': 379.0, 'min': -370.0, 'percentile_00_5': 160.0, 'percentile_99_5': 668.0, 'std': 105.92166900634766}}} \n",
      "\n",
      "2025-03-18 13:57:15.244949: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-03-18 13:57:15.267951: \n",
      "2025-03-18 13:57:15.269926: Epoch 0\n",
      "2025-03-18 13:57:15.272301: Current learning rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "export CUDA_VISIBLE_DEVICES=5 \n",
    ".local/bin/nnUNetv2_train -device \"cuda\" 100 3d_fullres 0 --num_epochs 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
